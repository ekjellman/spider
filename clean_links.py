# -*- coding:utf-8 -*-

"""
Most of the code in this module is modified from functions in spider.py,
but this is meant to be standalone for now.

clean_links.py takes a links.txt file generated by the spider and cleans
it up, removing Add: links over a certain threshold, and trying to keep
a sample of links from a variety of domains.
"""

# TODO: Modify this and spider.py to have shared functions

import sys
import collections
import urlparse
import random
MAX_QUEUE_SIZE = 10000
MAX_QUEUED_URLS_PER_DOMAIN = 10

def load_links(links):
  """Load links from a links.txt file."""
  visited = set()
  url_queue = []
  try:
    with open(links, "r") as link_file:
      lines = [line.strip() for line in link_file.readlines()]
      for line in lines:
        try:
          kind, url = line.split(":", 1)
        except:
          continue
        if kind == "Visit":
          visited.add(url)
        elif kind == "Add":
          if valid_link(url):
            url_queue.append(url)
        else:
          continue
    url_queue = trim_queue(url_queue)
    for url in visited:
      if url in url_queue:
        url_queue.remove(url)
  except IOError:
    return
  url_queue = trim_queue(url_queue)
  return (visited, url_queue)

def trim_queue(url_queue):
  """Try to remove urls from the queue, while ensuring some from each domain."""
  urls_by_domain = collections.defaultdict(list)
  for url in url_queue:
    domain = get_domain(url)
    urls_by_domain[domain].append(url)
  urls_per_domain = (MAX_QUEUE_SIZE / len(urls_by_domain)) + 1
  if urls_per_domain > MAX_QUEUED_URLS_PER_DOMAIN:
    urls_per_domain = MAX_QUEUED_URLS_PER_DOMAIN
  new_urls = []
  for domain in urls_by_domain:
    random.shuffle(urls_by_domain[domain])
    new_urls.extend(urls_by_domain[domain][:urls_per_domain])
  random.shuffle(new_urls)
  if len(new_urls) > MAX_QUEUE_SIZE:
    new_urls = new_urls[:MAX_QUEUE_SIZE]
  return new_urls

def get_domain(url):
  """Get url for a base domain from a url.
     Ex: https://www.a.com/foo/bar -> https://www.a.com/"""
  return urlparse.urljoin(url, "/")

def valid_link(url):
  """Returns False if a link appears to be to non-html, True otherwise."""
  invalid = [".jpg", ".jpeg", ".gif", ".png", ".zip", ".gz", ".7z",
             ".exe", ".bin", ".dmg", ".mp4", ".mp3", ".mov", ".qt",
             ".mkv", ".pdf"]
  for i in invalid:
    if url.endswith(i):
      return False
  return True

def write_new_links(visited, url_queue, out_file):
  """Write a links file for the given url_queue and visited set."""
  with open(out_file, "w") as file_out:
    for url in visited:
      file_out.write("Visit:%s\n" % url)
    for url in url_queue:
      file_out.write("Add:%s\n" % url)

visited, url_queue = load_links(sys.argv[1])
write_new_links(visited, url_queue, sys.argv[2])
